{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e5aa15c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from qkeras import QActivation,QConv2D,QDense,quantized_bits\n",
    "import qkeras\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "from telescope import *\n",
    "from utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12f1a146",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-10 04:20:04.355011: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2000, 18), dtype=float32, numpy=\n",
       "array([[-0.6484375 , -0.8828125 ,  1.484375  , ...,  1.203125  ,\n",
       "        -0.22449946,  0.72620314],\n",
       "       [-0.0859375 , -1.5       ,  1.9921875 , ..., -0.25      ,\n",
       "         0.41269925,  1.3830566 ],\n",
       "       [-0.890625  ,  0.328125  ,  1.890625  , ...,  0.1171875 ,\n",
       "         1.1381445 , -1.1917657 ],\n",
       "       ...,\n",
       "       [ 0.34375   , -1.8046875 ,  1.890625  , ...,  0.9453125 ,\n",
       "        -0.2736338 ,  0.5584123 ],\n",
       "       [ 0.765625  , -0.5546875 , -0.0625    , ...,  0.640625  ,\n",
       "        -0.8595276 , -0.51018775],\n",
       "       [-0.5       , -0.875     , -0.046875  , ...,  1.8046875 ,\n",
       "        -1.0860186 ,  1.8798432 ]], dtype=float32)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder([tf.random.normal([2000, 9, 9, 1]),tf.random.normal([2000,1]),tf.random.normal([2000,1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d534cd2f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer 'conv2d' (type QConv2D).\n\nAttempt to convert a value (TensorSpec(shape=(2000, 9, 9, 1), dtype=tf.float32, name=None)) with an unsupported type (<class 'tensorflow.python.framework.tensor.TensorSpec'>) to a Tensor.\n\nCall arguments received by layer 'conv2d' (type QConv2D):\n  • inputs=TensorSpec(shape=(2000, 9, 9, 1), dtype=tf.float32, name=None)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensorSpec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensorSpec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensorSpec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/uscms_data/d3/nswood/mambaforge/envs/myenv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/uscms_data/d3/nswood/mambaforge/envs/myenv/lib/python3.10/site-packages/qkeras/qconvolutional.py:294\u001b[0m, in \u001b[0;36mQConv2D.call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    292\u001b[0m   quantized_kernel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel\n\u001b[0;32m--> 294\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantized_kernel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdilation_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_bias:\n\u001b[1;32m    303\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_quantizer:\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer 'conv2d' (type QConv2D).\n\nAttempt to convert a value (TensorSpec(shape=(2000, 9, 9, 1), dtype=tf.float32, name=None)) with an unsupported type (<class 'tensorflow.python.framework.tensor.TensorSpec'>) to a Tensor.\n\nCall arguments received by layer 'conv2d' (type QConv2D):\n  • inputs=TensorSpec(shape=(2000, 9, 9, 1), dtype=tf.float32, name=None)"
     ]
    }
   ],
   "source": [
    "encoder([tf.TensorSpec(model.inputs[0].shape, model.inputs[0].dtype),\n",
    "                                              tf.TensorSpec(model.inputs[1].shape, model.inputs[1].dtype),\n",
    "                                              tf.TensorSpec(model.inputs[2].shape, model.inputs[2].dtype)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2cf6c824",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = encoder\n",
    "@tf.function\n",
    "def full_model(x,y,z):\n",
    "    return model([x,y,z])\n",
    "\n",
    "full_model = full_model.get_concrete_function(tf.TensorSpec(model.inputs[0].shape, model.inputs[0].dtype),\n",
    "                                              tf.TensorSpec(model.inputs[1].shape, model.inputs[1].dtype),\n",
    "                                              tf.TensorSpec(model.inputs[2].shape, model.inputs[2].dtype)\n",
    "                                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26584cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-10 04:16:50.976736: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-10 04:16:53.114599: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 20.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-10 04:16:57.924537: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10823 MB memory:  -> device: 0, name: Tesla P100-PCIE-12GB, pci bus id: 0000:65:00.0, compute capability: 6.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define a function with multiple input arguments\n",
    "@tf.function\n",
    "def add_and_multiply(a, b, c):\n",
    "    return (a + b) * c\n",
    "\n",
    "# Create representative input tensors\n",
    "a_rep = tf.constant(2.0)\n",
    "b_rep = tf.constant(3.0)\n",
    "c_rep = tf.constant(4.0)\n",
    "\n",
    "# Get the concrete function\n",
    "concrete_func = add_and_multiply.get_concrete_function(a_rep, b_rep, c_rep)\n",
    "\n",
    "# Test the concrete function\n",
    "result = concrete_func(a_rep, b_rep, c_rep)\n",
    "print(\"Result:\", result.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3edcc0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers_list = [\n",
    "    0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 12, 13, 16, 17, 18, 19, 20, 21, 22, 23, 24, \n",
    "    25, 28, 29, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 44, 45, 48, 49, 50, 51, \n",
    "    52, 53, 54, 55, 56, 57, 60, 61, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 76, \n",
    "    77, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 92, 93, 96, 97, 98, 99, 100, 101, \n",
    "    102, 103, 104, 105, 108, 109, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 124, 125\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49169891",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/rohanshenoy/Ecoder/blob/628fef43f1d5803999b4f30f7157f284fdb7c816/qDenseCNN.py#L136\n",
    "def GetQbits(self, inp, keep_negative=1):\n",
    "    print(\"Setting bits {} {} with keep negative = {}\".format(inp['total'], inp['integer'], keep_negative))\n",
    "    b =  qkeras.quantized_bits(bits=inp['total'], integer=inp['integer'], keep_negative=keep_negative, alpha=1)\n",
    "    print('max = %s, min = %s'%(b.max(),b.min()))\n",
    "    print('str representation:%s'%(str(b)))\n",
    "    print('config = ',b.get_config())\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42009574",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_mse_loss(y_true, y_pred):\n",
    "    y_true = tf.matmul(K.reshape(y_true,(-1,81)),remap_9x9_matrix)\n",
    "    y_pred = tf.matmul(K.reshape(y_pred,(-1,81)),remap_9x9_matrix)\n",
    "    # Calculate the squared difference between predicted and target values\n",
    "    squared_diff = tf.square(y_pred - y_true)\n",
    "\n",
    "    # Calculate the MSE per row (reduce_mean along axis=1)\n",
    "    mse_per_row = tf.reduce_mean(squared_diff, axis=1)\n",
    "\n",
    "    # Take the mean of the MSE values to get the overall MSE loss\n",
    "    mean_mse_loss = tf.reduce_mean(mse_per_row)\n",
    "\n",
    "    return mean_mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e510f7db",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"cae\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(2000, 9, 9, 1)]            0         []                            \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)        [(2000, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)        [(2000, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " encoder (Functional)        (2000, 18)                   2144      ['input_1[0][0]',             \n",
      "                                                                     'input_2[0][0]',             \n",
      "                                                                     'input_3[0][0]']             \n",
      "                                                                                                  \n",
      " decoder (Functional)        (2000, 9, 9, 1)              10449     ['encoder[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 12593 (49.19 KB)\n",
      "Trainable params: 12593 (49.19 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "batch = 2000\n",
    "\n",
    "#Specs\n",
    "n_kernels = 8\n",
    "n_encoded=16\n",
    "conv_weightBits  = 6 \n",
    "conv_biasBits  = 6 \n",
    "dense_weightBits  = 6 \n",
    "dense_biasBits  = 6 \n",
    "encodedBits = 9\n",
    "CNN_kernel_size = 3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "input_enc = Input(batch_shape=(batch,9,9, 1))\n",
    "sum_input = Input(batch_shape=(batch,1))\n",
    "eta = Input(batch_shape =(batch,1))\n",
    "x = QActivation(qkeras.quantized_bits(bits = 8, integer = 1),name = 'input_quantization')(input_enc)\n",
    "\n",
    "x = QConv2D(n_kernels,\n",
    "            CNN_kernel_size, \n",
    "#             padding='same',\n",
    "            strides=2,\n",
    "            kernel_quantizer=quantized_bits(bits=conv_weightBits,integer=0,keep_negative=1,alpha=1),\n",
    "            bias_quantizer=quantized_bits(bits=conv_biasBits,integer=0,keep_negative=1,alpha=1),\n",
    "            name=\"conv2d\")(input_enc)\n",
    "x = QActivation(\"quantized_relu(bits=8,integer=1)\", name=\"act\")(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "\n",
    "x = QDense(n_encoded, \n",
    "           kernel_quantizer=quantized_bits(bits=dense_weightBits,integer=0,keep_negative=1,alpha=1),\n",
    "           bias_quantizer=quantized_bits(bits=dense_biasBits,integer=0,keep_negative=1,alpha=1),\n",
    "           name=\"dense\")(x)\n",
    "x = QActivation(qkeras.quantized_bits(bits = 9, integer = 1),name = 'latent_quantization')(x)\n",
    "\n",
    "x = concatenate([x,sum_input,eta],axis=1)\n",
    "\n",
    "latent = x\n",
    "\n",
    "input_dec = Input(batch_shape=(batch,18))\n",
    "y = Dense(24)(input_dec)\n",
    "y = ReLU()(y)\n",
    "y = Dense(64)(y)\n",
    "y = ReLU()(y)\n",
    "y = Dense(128)(y)\n",
    "y = ReLU()(y)\n",
    "y = Reshape((4, 4, 8))(y)\n",
    "y = Conv2DTranspose(1, (3, 3), strides=(2, 2))(y)\n",
    "y = ReLU()(y)\n",
    "\n",
    "# y = tf.keras.activations.sigmoid(y)\n",
    "recon = y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "encoder = keras.Model([input_enc,sum_input,eta], latent, name=\"encoder\")\n",
    "decoder = keras.Model([input_dec], recon, name=\"decoder\")\n",
    "\n",
    "cae = Model(\n",
    "    inputs=[input_enc,sum_input,eta],\n",
    "    outputs=decoder([encoder([input_enc,sum_input,eta])]),\n",
    "    name=\"cae\"\n",
    ")\n",
    "#telescopeMSE9x9\n",
    "#mean_mse_loss\n",
    "cae.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = 1e-5,weight_decay = 0.00025), loss=mean_mse_loss)\n",
    "cae.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59e3373c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = pd.read_csv('../ECON_AE_Development/AE_Data/1.csv').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0471fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tensor = tf.convert_to_tensor(np.concatenate(data_list), dtype=tf.float32)\n",
    "data_tensor = data_tensor[0:500000]\n",
    "train_size = int(0.8 * len(data_tensor))\n",
    "test_size = len(data_tensor) - train_size\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_data, test_data = tf.split(data_tensor, [train_size, test_size], axis=0)\n",
    "\n",
    "# Extract specific tensors\n",
    "if normalize:\n",
    "    train_sum_calcq = tf.expand_dims(tf.reduce_sum(train_data[:, 0:48], axis=1), axis=1)\n",
    "    train_data = tf.boolean_mask(train_data,tf.squeeze(train_sum_calcq,axis=-1) != 0)\n",
    "    train_sum_calcq = tf.boolean_mask(train_sum_calcq,tf.squeeze(train_sum_calcq,axis=-1) != 0)\n",
    "    train_wafers = expand_tensor(train_data[:, 0:48]/train_sum_calcq)\n",
    "else:\n",
    "    train_sum_calcq = tf.expand_dims(tf.reduce_sum(train_data[:, 0:48], axis=1), axis=1)\n",
    "    train_wafers = expand_tensor(train_data[:, 0:48])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0a27b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available.\n",
      "GPU device name: /device:GPU:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-01 16:26:33.862632: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /device:GPU:0 with 10823 MB memory:  -> device: 0, name: Tesla P100-PCIE-12GB, pci bus id: 0000:65:00.0, compute capability: 6.0\n",
      "2023-08-01 16:26:33.866730: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /device:GPU:0 with 10823 MB memory:  -> device: 0, name: Tesla P100-PCIE-12GB, pci bus id: 0000:65:00.0, compute capability: 6.0\n"
     ]
    }
   ],
   "source": [
    "if tf.test.is_gpu_available():\n",
    "    print(\"GPU is available.\")\n",
    "    print(\"GPU device name:\", tf.test.gpu_device_name())\n",
    "else:\n",
    "    print(\"GPU is not available.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97581c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(nfiles,batchsize, normalize = False):\n",
    "    ecr = np.vectorize(encode)\n",
    "    data_list = []\n",
    "\n",
    "    for i in range(nfiles):\n",
    "        if i == 0:\n",
    "            dt = pd.read_csv('../ECON_AE_Development/AE_Data/1.csv').values\n",
    "        else:\n",
    "            dt_i = pd.read_csv(f'../ECON_AE_Development/AE_Data/{i+1}.csv').values\n",
    "            dt = np.vstack([dt, dt_i])\n",
    "\n",
    "        data_list.append(dt)\n",
    "\n",
    "    data_tensor = tf.convert_to_tensor(np.concatenate(data_list), dtype=tf.float32)\n",
    "    data_tensor = data_tensor[0:500000]\n",
    "    train_size = int(0.8 * len(data_tensor))\n",
    "    test_size = len(data_tensor) - train_size\n",
    "\n",
    "    # Split the data into training and test sets\n",
    "    train_data, test_data = tf.split(data_tensor, [train_size, test_size], axis=0)\n",
    "\n",
    "    # Extract specific tensors\n",
    "    if normalize:\n",
    "        train_sum_calcq = tf.expand_dims(tf.reduce_sum(train_data[:, 0:48], axis=1), axis=1)\n",
    "        train_data = tf.boolean_mask(train_data,tf.squeeze(train_sum_calcq,axis=-1) != 0)\n",
    "        train_sum_calcq = tf.boolean_mask(train_sum_calcq,tf.squeeze(train_sum_calcq,axis=-1) != 0)\n",
    "        train_wafers = expand_tensor(train_data[:, 0:48]/train_sum_calcq)\n",
    "    else:\n",
    "        train_sum_calcq = tf.expand_dims(tf.reduce_sum(train_data[:, 0:48], axis=1), axis=1)\n",
    "        train_wafers = expand_tensor(train_data[:, 0:48])\n",
    "    \n",
    "    \n",
    "\n",
    "    if normalize:\n",
    "        test_sum_calcq = tf.expand_dims(tf.reduce_sum(test_data[:, 0:48], axis=1), axis=1)\n",
    "        test_data = tf.boolean_mask(test_data,tf.squeeze(test_sum_calcq,axis=-1) != 0)\n",
    "        test_sum_calcq = tf.boolean_mask(test_sum_calcq,tf.squeeze(test_sum_calcq,axis=-1) != 0)\n",
    "        test_wafers = expand_tensor(test_data[:, 0:48]/test_sum_calcq)\n",
    "    else:\n",
    "        test_sum_calcq = tf.expand_dims(tf.reduce_sum(test_data[:, 0:48], axis=1), axis=1)\n",
    "        test_wafers = expand_tensor(test_data[:, 0:48])\n",
    "    \n",
    "    ''' \n",
    "    \n",
    "    Quantize Sum CalcQ with 5 exp bits and 4 mant bits\n",
    "    \n",
    "    Taking the log here is effectivel taking the log on readout. It makes no change to the quantization,\n",
    "    just a convenient setup for keras.\n",
    "    \n",
    "    '''\n",
    "    train_sum_calcq = train_sum_calcq.numpy().astype(int)\n",
    "    train_sum_calcq = ecr(train_sum_calcq, dropBits=0, expBits=5, mantBits=4, roundBits=False, asInt=True)\n",
    "    train_sum_calcq = tf.math.log(tf.constant(train_sum_calcq,dtype = tf.float64))\n",
    "    train_eta = tf.expand_dims(train_data[:, -2], axis=1)\n",
    "    \n",
    "    test_sum_calcq = test_sum_calcq.numpy().astype(int)\n",
    "    test_sum_calcq = ecr(test_sum_calcq, dropBits=0, expBits=5, mantBits=4, roundBits=False, asInt=True)\n",
    "    test_sum_calcq = tf.math.log(tf.constant(test_sum_calcq,dtype = tf.float64))\n",
    "    test_eta = tf.expand_dims(test_data[:, -2], axis=1)\n",
    "\n",
    "    # Create data loaders for training and test data\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((train_wafers, train_sum_calcq, train_eta))\n",
    "    train_loader = train_dataset.batch(batchsize).shuffle(buffer_size=train_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((test_wafers, test_sum_calcq, test_eta))\n",
    "    test_loader = test_dataset.batch(batchsize).shuffle(buffer_size=test_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "    return train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c5576d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_tensor(input_tensor):\n",
    "    arrange = np.array([28,29,30,31,0,4,8,12,\n",
    "                         24,25,26,27,1,5,9,13,\n",
    "                         20,21,22,23,2,6,10,14,\n",
    "                         16,17,18,19,3,7,11,15,\n",
    "                         47,43,39,35,35,34,33,32,\n",
    "                         46,42,38,34,39,38,37,36,\n",
    "                         45,41,37,33,43,42,41,40,\n",
    "                             44,40,36,32,47,46,45,44])\n",
    "    arrMask = np.array([1,1,1,1,1,1,1,1,\n",
    "                        1,1,1,1,1,1,1,1,\n",
    "                        1,1,1,1,1,1,1,1,\n",
    "                        1,1,1,1,1,1,1,1,\n",
    "                        1,1,1,1,0,0,0,0,\n",
    "                        1,1,1,1,0,0,0,0,\n",
    "                        1,1,1,1,0,0,0,0,\n",
    "                        1,1,1,1,0,0,0,0,])\n",
    "   \n",
    "    inputdata = tf.reshape(tf.gather(input_tensor, arrange,axis =1), (input_tensor.shape[0],8, 8, 1))\n",
    "#     inputdata *= tf.cast(tf.reshape(arrMask, (1, 8, 8)), dtype=inputdata.dtype)\n",
    "\n",
    "    paddings = [(0, 0), (0, 1), (0, 1), (0, 0)]\n",
    "    padded_tensor = tf.pad(inputdata, paddings, mode='CONSTANT', constant_values=0)\n",
    "\n",
    "    return padded_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20355b47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_loader, test_loader = load_data(1,batch,normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13d32e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "remap_9x9 = [\n",
    "    4, 13, 22, 31, 5, 14, 23, 32, 6, 15, 24, 33, 7, 16, 25, 34,\n",
    "    27, 28, 29, 30, 18, 19, 20, 21, 9, 10, 11, 12, 0, 1, 2, 3,\n",
    "    66, 57, 48, 40, 65, 56, 50, 49, 64, 60, 59, 58, 70, 69, 68, 67\n",
    "]\n",
    "\n",
    "remap_9x9_matrix = np.zeros(48*81,dtype=np.float32).reshape((81,48))\n",
    "\n",
    "for i in range(48): \n",
    "    remap_9x9_matrix[remap_9x9[i],i] = 1\n",
    "# def telescopeMSE9x9(y_true,y_pred):\n",
    "#     return telescopeMSE2(tf.matmul(K.reshape(y_true,(-1,81)),remap_9x9_matrix),\n",
    "#                          tf.matmul(K.reshape(y_pred,(-1,81)),remap_9x9_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ed8afd10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dif = tf.matmul(K.reshape(cae([wafers, sum_calcq, eta]),(-1,81)),remap_9x9_matrix)-tf.matmul(K.reshape(wafers,(-1,81)),remap_9x9_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3fca1300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2000, 2000), dtype=float32, numpy=\n",
       "array([[0.88447297, 0.22068278, 0.7167299 , ..., 1.0648726 , 0.8572591 ,\n",
       "        0.60360783],\n",
       "       [0.51027286, 0.12731698, 0.41349798, ..., 0.6143496 , 0.49457258,\n",
       "        0.34823528],\n",
       "       [6.6335473 , 1.6551208 , 5.375474  , ..., 7.986545  , 6.4294434 ,\n",
       "        4.5270586 ],\n",
       "       ...,\n",
       "       [1.8952992 , 0.47289166, 1.5358497 , ..., 2.2818701 , 1.8369838 ,\n",
       "        1.2934453 ],\n",
       "       [1.6583868 , 0.4137802 , 1.3438685 , ..., 1.9966363 , 1.6073608 ,\n",
       "        1.1317647 ],\n",
       "       [0.60304976, 0.15046553, 0.48867944, ..., 0.72604954, 0.5844948 ,\n",
       "        0.4115508 ]], dtype=float32)>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(dif,axis = [1])/sum_calcq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "52b11b36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 48), dtype=float32, numpy=\n",
       "array([[0.        , 0.        , 0.1350457 , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.0309369 ,\n",
       "        0.        , 0.        , 0.00258605, 0.        , 0.04333076,\n",
       "        0.25556087, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.0372609 ,\n",
       "        0.        , 0.01337619, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.18429805,\n",
       "        0.        , 0.        , 0.3622913 , 0.        , 0.        ,\n",
       "        0.03496099, 0.        , 0.        ]], dtype=float32)>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.matmul(K.reshape(cae([wafers, sum_calcq, eta])[i],(-1,81)),remap_9x9_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "6dfbf334",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 48), dtype=float32, numpy=\n",
       "array([[0.        , 0.        , 0.08648649, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.16216215, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.2       , 0.16216215, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.38918918, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ]], dtype=float32)>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.matmul(K.reshape(wafers[i],(-1,81)),remap_9x9_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "faccf799",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-01 15:09:40.825128: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape ingradient_tape/cae/encoder/act/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 6.043301221216097e-06\n",
      "Epoch 2/50, Loss: 1.492909830994904e-07\n",
      "Epoch 3/50, Loss: 1.7512552556581796e-06\n",
      "Epoch 4/50, Loss: 9.572969051077962e-08\n",
      "Epoch 5/50, Loss: 4.921620690729469e-06\n",
      "Epoch 6/50, Loss: 4.025467851897702e-06\n",
      "Epoch 7/50, Loss: 2.377213683212176e-06\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wafers\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m batch:\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mcae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_on_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mwafers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msum_calcq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meta\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwafers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     total_loss \u001b[38;5;241m=\u001b[39mtotal_loss \u001b[38;5;241m+\u001b[39m loss\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;241m/\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;241m*\u001b[39mbatch)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/uscms_data/d3/nswood/mambaforge/envs/myenv/lib/python3.10/site-packages/keras/src/engine/training.py:2684\u001b[0m, in \u001b[0;36mModel.train_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[1;32m   2680\u001b[0m     iterator \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39msingle_batch_iterator(\n\u001b[1;32m   2681\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy, x, y, sample_weight, class_weight\n\u001b[1;32m   2682\u001b[0m     )\n\u001b[1;32m   2683\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_train_function()\n\u001b[0;32m-> 2684\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2686\u001b[0m logs \u001b[38;5;241m=\u001b[39m tf_utils\u001b[38;5;241m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[1;32m   2687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict:\n",
      "File \u001b[0;32m/uscms_data/d3/nswood/mambaforge/envs/myenv/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/uscms_data/d3/nswood/mambaforge/envs/myenv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/uscms_data/d3/nswood/mambaforge/envs/myenv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    854\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    855\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    856\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 857\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    859\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    860\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/uscms_data/d3/nswood/mambaforge/envs/myenv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m    147\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/uscms_data/d3/nswood/mambaforge/envs/myenv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1348\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1350\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m     args,\n\u001b[1;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1353\u001b[0m     executing_eagerly)\n\u001b[1;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/uscms_data/d3/nswood/mambaforge/envs/myenv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlist\u001b[39m(args))\n",
      "File \u001b[0;32m/uscms_data/d3/nswood/mambaforge/envs/myenv/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1460\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1461\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1465\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1466\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1467\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1472\u001b[0m   )\n",
      "File \u001b[0;32m/uscms_data/d3/nswood/mambaforge/envs/myenv/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# cae.compile(optimizer=tf.keras.optimizers.Lion(learning_rate = 5e-3,weight_decay = 0.00025), loss=telescopeMSE9x9)\n",
    "\n",
    "nepochs = 50\n",
    "for epoch in range(nepochs):\n",
    "    total_loss = 0\n",
    "    for wafers, sum_calcq, eta in train_loader:\n",
    "        # Train the CAE model on each batch\n",
    "\n",
    "        if wafers.shape[0] != batch:\n",
    "            break\n",
    "        loss = cae.train_on_batch([wafers, sum_calcq, eta], wafers)\n",
    "        total_loss =total_loss + loss\n",
    "        \n",
    "    print(f\"Epoch {epoch+1}/{nepochs}, Loss: {total_loss/(len(train_loader)*batch)}\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbeafcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(args):\n",
    "    data_list = []\n",
    "\n",
    "    for i in range(args.nfiles):\n",
    "        if i == 0:\n",
    "            dt = pd.read_csv('../ECON_AE_Development/AE_Data/1.csv').values\n",
    "        else:\n",
    "            dt_i = pd.read_csv(f'../ECON_AE_Development/AE_Data/{i+1}.csv').values\n",
    "            dt = np.vstack([dt, dt_i])\n",
    "\n",
    "        data_list.append(dt)\n",
    "\n",
    "    data_tensor = tf.convert_to_tensor(np.concatenate(data_list), dtype=tf.float32)\n",
    "\n",
    "    train_size = int(0.8 * len(data_tensor))\n",
    "    test_size = len(data_tensor) - train_size\n",
    "\n",
    "    # Split the data into training and test sets\n",
    "    train_data, test_data = tf.split(data_tensor, [train_size, test_size], axis=0)\n",
    "\n",
    "    # Extract specific tensors\n",
    "    train_wafers = train_data[:, 0:48]\n",
    "    train_sum_calcq = tf.reduce_sum(train_data[:, 0:48], axis=1)\n",
    "    train_eta = train_data[:, -2]\n",
    "\n",
    "    test_wafers = test_data[:, 0:48]\n",
    "    test_sum_calcq = tf.reduce_sum(test_data[:, 0:48], axis=1)\n",
    "    test_eta = test_data[:, -2]\n",
    "\n",
    "    # Create data loaders for training and test data\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((train_wafers, train_sum_calcq, train_eta))\n",
    "    train_loader = train_dataset.batch(args.batchsize).shuffle(buffer_size=train_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((test_wafers, test_sum_calcq, test_eta))\n",
    "    test_loader = test_dataset.batch(args.batchsize).shuffle(buffer_size=test_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "    return train_loader, test_loader\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
